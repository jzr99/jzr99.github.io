---
---
@inproceedings{DNF-Avatar,
author = {Jiang, Zeren and Wang, Shaofei and Tang, Siyu},
title = {DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting},
abstract = {Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, e.g. virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering (PBR), to estimate geometry and disentangle appearance properties of human avatars. However, one drawback of these methods is the slow rendering speed due to the expensive Monte Carlo ray tracing. To tackle this problem, we proposed to distill the knowledge from implicit neural fields (teacher) to explicit 2D Gaussian splatting (student) representation to take advantage of the fast rasterization property of Gaussian splatting. To avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We also propose novel part-wise ambient occlusion probes for shadow computation. Shadow prediction is achieved by querying these probes only once per pixel, which paves the way for real-time relighting of avatars. These techniques combined give high-quality relighting results with realistic shadow effects. Our experiments demonstrate that the proposed student model achieves comparable relighting results with our teacher model while being 370 times faster at inference time, achieving a 67 FPS rendering speed.},
booktitle = {ArXiv preprint},
month     = {April},
year      = {2025},
pdf={dnf-avatar.pdf},
website={https://jzr99.github.io/DNF-Avatar/},
selected={true},
preview={dnf-avatar-logo.png},
code={https://github.com/jzr99/DNF-Avatar/},
}

@inproceedings{Geo4D,
author = {Jiang, Zeren and Zheng, Chuanxia and Laina, Iro and Larlus, Diane and Vedaldi, Andrea},
title = {Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction},
abstract = {We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.},
booktitle = {ArXiv preprint},
month     = {April},
year      = {2025},
pdf={geo4d_v1.pdf},
website={https://geo4d.github.io/},
selected={true},
preview={geo4d_logo.png},
code={https://github.com/jzr99/Geo4D/},
}

@inproceedings{MultiPly,
author = {Jiang*, Zeren and Guo*, Chen and Kaufmann, Manuel and Jiang, Tianjian and Valentin, Julien and Hilliges, Otmar and Song, Jie},
title = {MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild },
abstract = {We present MultiPly, a novel framework to reconstruct multiple people in 3D from monocular in-the-wild videos. Reconstructing multiple individuals moving and interacting naturally from monocular in-the-wild videos poses a challenging task. Addressing it necessitates precise pixel-level disentanglement of individuals without any prior knowledge about the subjects. Moreover, it requires recovering intricate and complete 3D human shapes from short video sequences, intensifying the level of difficulty. To tackle these challenges, we first define a layered neural representation for the entire scene, composited by individual human and background models. We learn the layered neural representation from videos via our layer-wise differentiable volume rendering. This learning process is further enhanced by our hybrid instance segmentation approach which combines the self-supervised 3D segmentation and the promptable 2D segmentation module, yielding reliable instance segmentation supervision even under close human interaction. A confidence-guided optimization formulation is introduced to optimize the human poses and shape/appearance alternately. We incorporate effective objectives to refine human poses via photometric information and impose physically plausible constraints on human dynamics, leading to temporally consistent 3D reconstructions with high fidelity. The evaluation of our method shows the superiority over prior art on publicly available datasets and in-the-wild videos.},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month     = {June},
year      = {2024},
pdf={multiply.pdf},
website={https://eth-ait.github.io/MultiPly/},
selected={true},
award={Oral},
preview={multiply.png},
code={https://github.com/eth-ait/MultiPly},
}

@InProceedings{10.1007/978-3-031-20077-9_19,
author="Wang, Luting
and Li, Xiaojie
and Liao, Yue
and Jiang, Zeren
and Wu, Jianlong
and Wang, Fei
and Qian, Chen
and Liu, Si",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="HEAD: HEtero-Assists Distillation for Heterogeneous Object Detectors",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
month="October",
publisher="Springer Nature Switzerland",
address="Cham",
pages="314--331",
abstract="Conventional knowledge distillation (KD) methods for object detection mainly concentrate on homogeneous teacher-student detectors. However, the design of a lightweight detector for deployment is often significantly different from a high-capacity detector. Thus, we investigate KD among heterogeneous teacher-student pairs for a wide application. We observe that the core difficulty for heterogeneous KD (hetero-KD) is the significant semantic gap between the backbone features of heterogeneous detectors due to the different optimization manners. Conventional homogeneous KD (homo-KD) methods suffer from such a gap and are hard to directly obtain satisfactory performance for hetero-KD. In this paper, we propose the HEtero-Assists Distillation (HEAD) framework, leveraging heterogeneous detection heads as assistants to guide the optimization of the student detector to reduce this gap. In HEAD, the assistant is an additional detection head with the architecture homogeneous to the teacher head attached to the student backbone. Thus, a hetero-KD is transformed into a homo-KD, allowing efficient knowledge transfer from the teacher to the student. Moreover, we extend HEAD into a Teacher-Free HEAD (TF-HEAD) framework when a well-trained teacher detector is unavailable. Our method has achieved significant improvement compared to current detection KD methods. For example, on the MS-COCO dataset, TF-HEAD helps R18 RetinaNet achieve 33.9 mAP ({\$}{\$}+2.2{\$}{\$}+2.2), while HEAD further pushes the limit to 36.2 mAP ({\$}{\$}+4.5{\$}{\$}+4.5).",
isbn="978-3-031-20077-9",
selected={true},
preview={head.png},
pdf={head.pdf},
code={https://github.com/LutingWang/HEAD},
}

@inproceedings{10.1145/3474085.3475195,
author = {Di*, Shangzhe and Jiang*, Zeren and Liu, Si and Wang, Zhaokai and Zhu, Leyan and He, Zexin and Liu, Hongming and Yan, Shuicheng},
title = {Video Background Music Generation with Controllable Music Transformer },
year = {2021},
month = {October},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475195},
doi = {10.1145/3474085.3475195},
abstract = {In this work, we address the task of video background music generation. Some previous works achieve effective music generation but are unable to generate melodious music specifically for a given video, and none of them considers the video-music rhythmic consistency. To generate the background music that matches the given video, we first establish the rhythmic relationships between video and background music. In particular, we connect timing, motion speed, and motion saliency from video with beat, simu-note density, and simu-note strength from music, respectively. We then propose CMT, a Controllable Music Transformer that enables the local control of the aforementioned rhythmic features, as well as the global control of the music genre and the used instrument specified by users. Objective and subjective evaluations show that the generated background music has achieved satisfactory compatibility with the input videos, and at the same time, impressive music quality.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2037–2045},
numpages = {9},
keywords = {video background music generation, transformer, music representation},
location = {Virtual Event, China},
series = {MM '21},
selected={true},
preview={cmt.png},
pdf={cmt.pdf},
website={https://wzk1015.github.io/cmt/},
award={Best Paper Award},
code={https://github.com/wzk1015/video-bgm-generation},
}

@InProceedings{Dai_2021_CVPR,
    author    = {Dai*, Xing and Jiang*, Zeren and Wu, Zhao and Bao, Yiping and Wang, Zhicheng and Liu, Si and Zhou, Erjin},
    title     = {General Instance Distillation for Object Detection},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {7842-7851},
    dimensions={false},
    selected={true},
    preview={gid.png},
    pdf={gid.pdf},
    abstract={In recent years, knowledge distillation has been proved to be an effective solution for model compression. This approach can make lightweight student models acquire the knowledge extracted from cumbersome teacher models. However, previous distillation methods of detection have weak generalization for different detection frameworks and rely heavily on ground truth (GT), ignoring the valuable relation information between instances. Thus, we propose a novel distillation method for detection tasks based on discriminative instances without considering the positive or negative distinguished by GT, which is called general instance distillation (GID). Our approach contains a general instance selection module (GISM) to make full use of feature-based, relation-based and response-based knowledge for distillation. Extensive results demonstrate that the student model achieves significant AP improvement and even outperforms the teacher in various detection frameworks. Specifically, RetinaNet with ResNet-50 achieves 39.1% in mAP with GID on COCO dataset, which surpasses the baseline 36.2% by 2.9%, and even better than the ResNet-101 based teacher model with 38.1% AP.}, 
}